import requests
from bs4 import BeautifulSoup
import socket
import subprocess
import re
import threading
import time
import random

# Define the proxy server's address and port
proxy_host = '127.0.0.1'
proxy_port = 8888

# Define global variables for the archive and lock
archive = []
archive_lock = threading.Lock()

# Function to measure latency to a given host using ICMP ping
def measure_latency(host):
    try:
        output = subprocess.check_output(['ping', '-c', '3', host])
        latency = re.search(r'min/avg/max/[^/]+ ms', output.decode())
        if latency:
            return float(latency.group().split('/')[4])
    except subprocess.CalledProcessError:
        pass
    return float('inf')  # Return infinity if ping fails

# Function to discover and select the best network from the archive
def select_network():
    global archive
    with archive_lock:
        random.shuffle(archive)  # Shuffle the archive to select a random network
        best_network = None
        best_latency = float('inf')

        for network in archive:
            latency = measure_latency(network)
            if latency < best_latency:
                best_network = network
                best_latency = latency

    return best_network

# Function to periodically refresh and recycle the archive of networks
def refresh_archive():
    global archive
    while True:
        time.sleep(3600)  # Refresh every hour
        # Scrape VPN servers from Google search results and update the archive
        with archive_lock:
            archive = scrape_vpn_servers()

# Function to scrape VPN servers from Google search results
def scrape_vpn_servers():
    vpn_servers = []
    query = "free VPN servers"
    url = f"https://www.google.com/search?q={query}"
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            results = soup.find_all('div', class_='BNeawe UPmit AP7Wnd')
            for result in results:
                vpn_servers.append(result.text)
    except Exception as e:
        print(f"Error scraping VPN servers: {e}")
    return vpn_servers

# Create a socket to listen for incoming connections
proxy_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
proxy_socket.bind((proxy_host, proxy_port))
proxy_socket.listen(1)

print(f"Proxy server listening on {proxy_host}:{proxy_port}")

# Initialize the archive with initial set of networks (for simulation purposes)
archive = scrape_vpn_servers()

# Start a separate thread to periodically refresh the archive
refresh_thread = threading.Thread(target=refresh_archive)
refresh_thread.start()

while True:
    # Select the best network from the archive
    selected_network = select_network()
    if selected_network:
        print(f"Selected network: {selected_network}")
    else:
        print("No available networks found.")
        time.sleep(30)  # Wait for 30 seconds before trying again
        continue

    # Accept incoming connections
    client_socket, client_address = proxy_socket.accept()
    print(f"Connection from {client_address}")

    # Receive data from the client
    data = client_socket.recv(4096)

    # Modify the data or route it through the selected network here

    # Send modified data back to the client
    client_socket.send(data)

    # Close the connection
    client_socket.close()